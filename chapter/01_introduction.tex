\chapter{Introduction}


\section{Motivation and goal}
%Cloud computing
With the advent of cloud computing platforms such as Amazon Web Services (AWS), Google Cloud Platform (GCP) or Microsoft Azure, the supply of dynamically scalable compute resources have changed.
To ensure the scalability of an application, this can be used to dynamically create new resources in case of a large load on an existing system.
This requires a dynamic environment in which new servers can be automatically integrated into an existing infrastructure.
Among other things, network configurations and server provisioning must be ensured.
However, classical systems are often not built to scale across multiple hosts and must be reinvented or adapted accordingly.
\\
%cloud native
To simplify the complexity of managing compute resources, as well as deploying and scaling an application, container orchestration systems were built with the advent of Docker\footnote{https://en.wikipedia.org/wiki/Docker\_(software)}.
One of these systems, which has found great acceptance in the market, is Kubernetes.
It follows the cloud-native\footnote{https://github.com/cncf/toc/blob/main/DEFINITION.md} approach, which is due to a constantly changing environment.
In the following, the terms cluster refers to a Kubernetes cluster and nodes are the number of servers that are grouped together in a cluster.
In a production environment, furthermore, it is recommended to run Kubernetes in a highly available\footnote{https://en.wikipedia.org/wiki/High\_availability} setup.
Kubernetes provides a failover mechanism in case a master node crashes and ensures high availabilities.
In the event of a crash, or once the number of nodes in a cluster changes, Kubernetes ensures that the application can continue to run, if there is at least one node available.
With Kubernetes, it is therefore comparatively easy to distribute an application across multiple servers and ensure scalability.
\\
%Load Balancers in cloud native
With the potentially ever-changing number of nodes, the question of load balancing also arises.
Typically, load balancers are used to serve as entry points and distribute traffic across multiple servers.
In a cloud-native environment, the configuration of a load balancer must also adapt to the constantly changing environment.
\\
Kubernetes provides a basis to dynamically create, delete and adapt an external load balancer for the cluster.
However, it does not offer an out-of-the-box implementation of load balancers for clusters.
The implementations of such load balancers are only available at various IaaS platforms (GCP, AWS, Azure).
This results in the need for a custom load balancer integration for bare-metal clusters.
\\
%KKP
Kubermatic GmbH sells the Kubermatic Kubernetes Platform\footnote{https://www.kubermatic.com/products/kubermatic/}, which extends the cloud-native approach to Kubernetes clusters.
The platform can be used to dynamically create, delete and modify clusters.
With the advent of large scale deployments comprising multiple clusters, there is a need for a centralized load balancing solution for dynamically created clusters in a bare metal environment.
There are several solutions for implementing load balancers, but these are designed for one cluster and usually require their own network setup.
The Project KubeLB\footnote{https://github.com/kubermatic/kubelb} aims to fill this gap and provide a load balancer integration for multiple clusters using the advantages of Kubernetes itself.

\section{Structure of this work}
Following the introduction, the basics of cloud-native and Kubernetes, as well as load balancing are first laid.
In the following chapter, the function of load balancers and the included network components and options with respect to Kubernetes will be discussed in more detail.
Chapter four discusses concepts and options within Kubernetes for extending functionality.
Subsequently, chapter five analyzes the concepts and transferability of the Kubermatic Kubernetes Platform and presents the KubeLB proposal.
In the sixth chapter, the distribution of responsibilities among subsystems and a mechanism for communication between the individual components is described.
The interaction of the components is then modeled based on the underlying features of layer 4 and layer 7 load balancing.
In conclusion, the final load balancer choice is discussed.
Chapter seven describes the implementation of the components developed in chapter six and their integration into Kubernetes.
Following the implementation, software tests will be described.
The final chapter of the thesis draws a conclusion with regard to the technologies used and the architectural design decisions.
This is followed in the last chapter by an outlook on further features that have not yet been implemented.
